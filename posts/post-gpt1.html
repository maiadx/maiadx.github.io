<link rel="stylesheet" href="style-post.css">
<div class="content-pane">

    <a href="../index.html" class="back-arrow">
        ‚Üê Back
    </a>

    <section>
        <h2>Building a GPT Model</h2>
        
        <a href="https://github.com/maiadx/AI_Learning/tree/main/gpt1" target="_blank" class="github-link">
            <img src="../res/github.png" alt="GitHub Profile" class="github-icon"> View Project on GitHub
        </a>
        <p>Started with n-gram model for simplicity, then moved to transformers to get significantly better quality output.</p>
    </section>

    <section>
        <h4>Important Hyperparameters</h4>
        <ul class="hyperparameters">
            <li><strong>Batch size:</strong> Number of independent text sequences processed concurrently.</li>
            <li><strong>Block size:</strong> Maximum context length for predictions.</li>
            <li><strong>Max iters:</strong> Number of training iterations.</li>
            <li><strong>Learning rate:</strong> Self-explanatory.</li>
            <li><strong>Embedding dimension:</strong> 384-dimensional table for 32-dimensional embeddings.</li>
        </ul>
    </section>

    <section>
        <h4>Encoder and Decoder</h4>
        <p><strong>Encoder:</strong> Creates a dictionary to map each token to an integer value for model learning.</p>
        <p><strong>Decoder:</strong> Maps integer values back to text tokens.</p>
    </section>

    <section>
        <h4>Attention Head</h4>
        <p><em>Attention</em> is the core component of GPT-1, providing model interpretability and sequence understanding.</p>
    </section>

    <details>
        <summary>The Code</summary>
        <pre class="code-block"><code>
class AttentionHead(nn.Module):
    def __init__(self, head_size):
        self.key = nn.Linear(n_embed_dim, head_size, bias=False)
        self.query = nn.Linear(n_embed_dim, head_size, bias=False)
        self.value = nn.Linear(n_embed_dim, head_size, bias=False)

    def forward(self, x):
        weights = softmax(query @ key.T)  # Apply softmax activation
        return weights @ value  # Aggregate values
        </code></pre>

        <pre class="code-block"><code>
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(head_size * num_heads, n_embed_dim)
        self.dropout = Dropout(p=0.1)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        return self.dropout(self.proj(out))
        </code></pre>

        <pre class="code-block"><code>
class FeedForwardNN(nn.Module):
    def __init__(self, n_embed_dim):
        self.net = nn.Sequential(
            nn.Linear(n_embed_dim, 4 * n_embed_dim),
            nn.ReLU(),
            nn.Linear(4 * n_embed_dim, n_embed_dim),
            nn.Dropout(p=0.1)
        )

    def forward(self, x):
        return self.net(x)
        </code></pre>

        <pre class="code-block"><code>
class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size, n_embed_dim, n_layer):
        self.token_embedding_table = nn.Embedding(vocab_size, n_embed_dim)
        self.position_embedding_table = nn.Embedding(block_size, n_embed_dim)
        self.blocks = nn.Sequential(*[Block(n_embed_dim, n_head) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embed_dim)  # Final layer norm
        self.lm_head = nn.Linear(n_embed_dim, vocab_size)
        self.apply(self._init_weights)

    def forward(self, idx, targets=None):
        tok_emb = self.token_embedding_table(idx)  # (B,T,C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)
        x = self.blocks(tok_emb + pos_emb)
        return self.lm_head(self.ln_f(x))  # (B,T,vocab_size)
        </code></pre>
    </details>
    
    <br /> <i>...notes here are not complete. Working on transferring all my notes and projects here!</i>
    
    <footer>
        <p>Last updated: October 26, 2024</p>
    </footer>

</div>
