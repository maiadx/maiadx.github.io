<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="preload" href="../res/image.jpg" as="image">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Maia Doxtader</title>
    <link rel="stylesheet" href="style-post.css">
</head>
<body>
    <div class="background-image"></div> <!-- Background image div -->

    <div class="content-pane">
        <a href="https://maiadx.me" class="back-arrow">
            ‚Üê Back
        </a>

        <section>
            <h2>'Let's Build GPT from Scratch'</h2>

            <a href="https://github.com/maiadx/AI_Learning/tree/main/gpt1" target="_blank" class="github-link">
                <img src="../res/github.png" alt="GitHub Profile" class="github-icon"> View Project on GitHub
            </a>
            <p>Started with n-gram model for simplicity, then moved to transformers to get significantly better quality output.</p>
        </section>

        <section>
            <h4>Important Hyperparameters</h4>
            <ul class="hyperparameters">
                <li><strong>Batch size:</strong> Number of independent text sequences processed concurrently.</li>
                <li><strong>Block size:</strong> Maximum context length for predictions.</li>
                <li><strong>Max iters:</strong> Number of training iterations.</li>
                <li><strong>Learning rate:</strong> Self-explanatory.</li>
                <li><strong>Embedding dimension:</strong> 384-dimensional table for 32-dimensional embeddings.</li>
            </ul>
        </section>

        <section>
            <h4>Encoder and Decoder</h4>
            <p><strong>Encoder:</strong> Creates a dictionary to map each token to an integer value for model learning.</p>
            <p><strong>Decoder:</strong> Maps integer values back to text tokens.</p>
        </section>

        <section>
            <h4>Attention Head</h4>
            <p><em>Attention</em> is the core component of GPT-1, providing model interpretability and sequence understanding.</p>
        </section>

        <details>
            <summary>The Code</summary>

            <pre><code class="language-python">
class AttentionHead(nn.Module):
    def __init__(self, head_size):
        self.key = nn.Linear(n_embed_dim, head_size, bias=False)
        self.query = nn.Linear(n_embed_dim, head_size, bias=False)
        self.value = nn.Linear(n_embed_dim, head_size, bias=False)

    def forward(self, x):
        weights = softmax(query @ key.T)  # Apply softmax activation
        return weights @ value  # Aggregate values
            </code></pre>

            <pre><code class="language-python">
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(head_size * num_heads, n_embed_dim)
        self.dropout = Dropout(p=0.1)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        return self.dropout(self.proj(out))
            </code></pre>

            <pre><code class="language-python">
class FeedForwardNN(nn.Module):
    def __init__(self, n_embed_dim):
        self.net = nn.Sequential(
            nn.Linear(n_embed_dim, 4 * n_embed_dim),
            nn.ReLU(),
            nn.Linear(4 * n_embed_dim, n_embed_dim),
            nn.Dropout(p=0.1)
        )

    def forward(self, x):
        return self.net(x)
            </code></pre>

            <pre><code class="language-python">
class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size, n_embed_dim, n_layer):
        token_embedding_table = Embedding(vocab_size, n_embed_dim)
        position_embedding_table = Embedding(block_size, n_embed_dim)
        blocks = Sequential(*[Block(n_embed_dim, n_head) for _ in range(n_layer)])
        ln_f = LayerNorm(n_embed_dim)  # Final layer norm
        lm_head = Linear(n_embed_dim, vocab_size)
        apply(_init_weights)

    def forward(self, idx, targets=None):
        tok_emb = self.token_embedding_table(idx)  # (B,T,C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)
        x = self.blocks(tok_emb + pos_emb)
        return self.lm_head(self.ln_f(x))  # (B,T,vocab_size)
            </code></pre>
        </details>

        <br /><i>...notes here are not complete. Working on transferring all my notes and projects here!</i>
        <h2>'Reproducing GPT-2'</h2>

        <footer>
            <p>Last updated: October 26, 2024</p>
        </footer>
    </div>

    <!-- Prism.js Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-rust.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
